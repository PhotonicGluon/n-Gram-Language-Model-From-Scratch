{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee840738-1a63-4c26-8c2d-7d40a0ca1b00",
   "metadata": {},
   "source": [
    "# $n$-Gram Shakespeare Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e768d006-375d-4eb8-b20a-a589edbd83ee",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff82649-c1db-4d48-827c-25861e01c869",
   "metadata": {},
   "source": [
    "General imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99ef3bfe-c14c-43f2-9f96-15f643139d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff8c1d5-a204-4f08-bb95-856903589966",
   "metadata": {},
   "source": [
    "General constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14b82e4b-1bf3-45b6-8556-16587ba1df2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS_FILE = \"data/shakespeare_input.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fae816a-90d4-4274-93bb-9ccc03b41707",
   "metadata": {},
   "source": [
    "Helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc8d3d83-9179-4f03-8472-fe3e72e470d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_n_lines(text: str, lines: int = 10):\n",
    "    print(\"\\n\".join(text.split(\"\\n\")[:lines]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b281554a-5852-4f33-ae91-3403c349f61e",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636bf525-5de8-45b7-b537-e22e56505413",
   "metadata": {},
   "source": [
    "We will be using the [Shakespeare Corpus](https://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt) from [Andrej Karpathy](https://cs.stanford.edu/people/karpathy/)'s [`char-rnn`](https://cs.stanford.edu/people/karpathy/char-rnn/) project. This corpus will be saved in the `data` folder as `shakespeare_input.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e061477c-976b-4161-a901-15a697e19399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus already exists\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isfile(CORPUS_FILE):\n",
    "    import urllib.request\n",
    "    \n",
    "    urllib.request.urlretrieve(\"https://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt\", CORPUS_FILE)\n",
    "    print(\"Downloaded corpus\")\n",
    "else:\n",
    "    print(\"Corpus already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9024a71a-f540-4a8d-8e72-7bd15fe73900",
   "metadata": {},
   "source": [
    "Let us read the first few lines of the raw corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3870926c-5402-4b9e-846c-00d2a1c17054",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(CORPUS_FILE, \"r\") as f:\n",
    "    raw_corpus = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "654422fa-f231-4a5f-b5e6-faf4feaa5449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n"
     ]
    }
   ],
   "source": [
    "print_n_lines(raw_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b5f720-3ddb-4295-a972-accf11053d18",
   "metadata": {},
   "source": [
    "We don't want the names of the speakers, and so we need to remove them.\n",
    "\n",
    "Notice that the speaker name ends with a colon, and is either\n",
    "- the first line; or\n",
    "- is preceeded by two newlines.\n",
    "\n",
    "We can thus make a RegEx to remove all speaker names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbc30226-b2ce-41c4-a1c5-e75f997de023",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_speakers(text: str) -> str:\n",
    "    # Remove first line\n",
    "    text = \"\\n\".join(text.split(\"\\n\")[1:])\n",
    "\n",
    "    # Use RegEx to identify subsequent speakers\n",
    "    text = re.sub(r\"\\n\\n.*:\", \"\", text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56739060-e06f-45f3-890e-e08e608d5bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "speakers_removed = remove_speakers(raw_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2238e044-ed6e-4ab8-b090-003969c11164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before we proceed any further, hear me speak.\n",
      "Speak, speak.\n",
      "You are all resolved rather to die than to famish?\n",
      "Resolved. resolved.\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "We know't, we know't.\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "No more talking on't; let it be done: away, away!\n",
      "One word, good citizens.\n"
     ]
    }
   ],
   "source": [
    "print_n_lines(speakers_removed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63567b0e-f881-4352-a802-99f48f957986",
   "metadata": {},
   "source": [
    "Now we can place all the lines of text in one line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5617699d-b5d5-4ced-a197-6b77647e2168",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \" \".join(speakers_removed.split(\"\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471b5318-cd54-46ae-b9c3-a2a42f738fa9",
   "metadata": {},
   "source": [
    "Convert the corpus to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d47b104a-9de2-4a0d-8228-2219443c42e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = corpus.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26d97581-3e34-4db5-82d6-ddb282d735bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before we proceed any further, hear me speak. speak, speak. you are all resolved rather to die than to famish? resolved. resolved. first, you know caius marcius is chief enemy to the people. we know't, we know't. let us kill him, and we'll have corn at our\n"
     ]
    }
   ],
   "source": [
    "print(corpus[:256])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fa563d-19e4-4090-af6e-d42a5e920f84",
   "metadata": {},
   "source": [
    "Split the corpus into words (and punctuation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "afa41f0f-44f2-4070-90e6-c7d1d42c3003",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = re.findall(r\"\\w+|[^\\w\\s]+\", corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8195553a-725c-4d6a-a1c8-fcc70ac290b6",
   "metadata": {},
   "source": [
    "Finally, remove any punctuation. These will be our tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3c98e0a-efda-4408-8e9a-474475fd1da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [word for word in words if word.isalpha()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2bcc4a-c8f0-49c1-a529-d535ea1aeb37",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a830de27-34d9-4316-ab93-55657b68ee6c",
   "metadata": {},
   "source": [
    "### Getting Sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302a418d-8c39-47cb-83fb-fe4454699c8a",
   "metadata": {},
   "source": [
    "Define the delimiters for the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2f3a823-654a-4bdf-b22c-8ffb2d4244f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTENCE_DELIMITERS = list(\".?!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3adce3a-e638-4f54-b092-a9e6dc331e9b",
   "metadata": {},
   "source": [
    "Get the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae417314-0a7c-4726-9820-cc480bf9fc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_pattern = \"|\".join(map(re.escape, SENTENCE_DELIMITERS))\n",
    "sentences = re.split(split_pattern, corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c50039-fd9d-41e1-ae35-9ab4bdbd6bfc",
   "metadata": {},
   "source": [
    "Strip any leading or trailing whitespace from the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60f41634-5f60-49d5-b521-50886caf2df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [sentence.strip() for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c74b29fe-2b5a-4b77-8e35-8d85ca84db60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['before we proceed any further, hear me speak', 'speak, speak', 'you are all resolved rather to die than to famish', 'resolved', 'resolved']\n"
     ]
    }
   ],
   "source": [
    "print(sentences[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cbb4e6-cd86-4b85-a4af-8feac16dc89c",
   "metadata": {},
   "source": [
    "### Building Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbbead4-2082-463e-a82a-3e35a98f0353",
   "metadata": {},
   "source": [
    "We will be using the set of all tokens as our vocabulary, although we will add three more tokens:\n",
    "- `<s>` for the start of the sentence;\n",
    "- `</s>` for the end of the sentence; and\n",
    "- `<unk>` for an unknown token (i.e., out of vocabulary (OOV) words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f949d5fb-6c1f-4d49-b09b-71233f0a8506",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "484a2c7f-9b0f-4509-9e6d-eee67441f661",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.add(\"<s>\")\n",
    "vocab.add(\"</s>\")\n",
    "vocab.add(\"<unk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9294b3-6202-4ace-b07b-b14ac02aad12",
   "metadata": {},
   "source": [
    "How large is our vocabulary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "043f7486-9e82-4705-9dee-0beff4c56651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22549"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24494a97-4971-4e96-8381-41f1f89645fd",
   "metadata": {},
   "source": [
    "Build a RegEx to match tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e99aca24-f516-4a74-8ad1-40d1c992cff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_by_length = sorted(list(vocab), key=lambda x: -len(x))  # Sorted by length in decreasing order\n",
    "vocab_re = re.compile(\"|\".join(map(re.escape, vocab_by_length)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dace87d-11e0-44f8-bfcc-b4df17c1f666",
   "metadata": {},
   "source": [
    "## Creating the $n$-Gram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027e384c-7631-4b4e-b619-3d1d0dcac766",
   "metadata": {},
   "source": [
    "Define the value of $n$ we will be using for our $n$-Gram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a43fc62f-020e-4c11-a63f-2394fb4bffe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff5549d-1fc1-4f06-966e-209f79a88e47",
   "metadata": {},
   "source": [
    "Get all the $n$-Grams from our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e4ad2129-8956-40a5-9873-93a7082b5edc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "575e17546a53470b8208014567aa9eef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing sentences:   0%|          | 0/52783 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from ngram import NGramModel\n",
    "\n",
    "model = NGramModel(N)\n",
    "\n",
    "for sentence in tqdm(sentences, desc=\"Processing sentences\"):\n",
    "    ngram = [\"<s>\"]\n",
    "    tokens = vocab_re.findall(sentence)\n",
    "    for word in tokens:\n",
    "        ngram.append(word)\n",
    "        if len(ngram) == N:\n",
    "            model.add_ngram(tuple(ngram))\n",
    "            ngram = ngram[1:]  # Remove first token\n",
    "\n",
    "    # Handle end of sentence case\n",
    "    ngram.append(\"</s>\")\n",
    "    if len(ngram) == N:\n",
    "        model.add_ngram(tuple(ngram))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484988b6-f875-4ffc-8ce9-6a004a46f63c",
   "metadata": {},
   "source": [
    "Save the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f94a867a-1d3b-44f1-ae29-b97ea41bb5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"models/shakespeare.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55190fe-d106-4fd7-b3ef-e5e8ada9fc6e",
   "metadata": {},
   "source": [
    "Load the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c062783e-7d35-4638-b552-6307f6596136",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_new = NGramModel.load(\"models/shakespeare.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e539812-5815-4244-983e-d9cb83325235",
   "metadata": {},
   "source": [
    "Test model generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d2975606-2236-4e76-a7cf-e966fbba795f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['great', 'sized', 'monster', 'of', 'you', 'should', 'bear', 'his', 'body', 'couched', 'in', 'thine', 'this', 'love', 'you', 'gainst', 'the', 'state', 'it', 'cannot', 'be', 'sounded', 'but', 'with', 'proviso', 'and', 'exception', 'that', 'we', 'did', 'not', 'gentle', 'eros', 'there', 'is', 'nothing', 'done', 'to', 'morrow', 'morning', 'lords', 'farewell', 'share', 'the', 'glory', 'is', 'to', 'tell', 'his', 'grace']\n"
     ]
    }
   ],
   "source": [
    "out = model_new.generate_text((\"<s>\", \"thou\"), 50, seed=8192, temperature=1)\n",
    "print(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
